{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8def34-fade-43cc-b2c5-605a8bc5c857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from torchvision.models.resnet import *\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b15a12-db33-4fef-bbed-28a70b7f7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b3045d-c421-414c-aaec-55994ef6e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-649d8fe959d11c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-649d8fe959d11c\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25744888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d12edf-672b-459b-9e83-9943f283a7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 07:38:05.910798: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6010f360-bb71-4b90-8ba8-d1d074259cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/facebook-marketplaces-recommendation-ranking-system/Practicals'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9734df-f22b-460a-9c87-142a45123c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a train dataset class\n",
    "class ItemsTrainDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.examples = self._load_examples()\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((225,225))\n",
    "        #self.rgbify = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "\n",
    "    def _load_examples(self):\n",
    "        class_names = os.listdir('pytorch_images_tv_split_2/train')\n",
    "        class_encoder = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        class_decoder = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "        examples_list = []\n",
    "        for cl_name in class_names:\n",
    "            example_fp = os.listdir(os.path.join('pytorch_images_tv_split_2/train',cl_name))\n",
    "            example_fp = [os.path.join('pytorch_images_tv_split_2/train', cl_name, img_name ) for img_name in example_fp]\n",
    "            example = [(img_name, class_encoder[cl_name]) for img_name in example_fp]\n",
    "            examples_list.extend(example)\n",
    "\n",
    "        return examples_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fp, img_class = self.examples[idx]\n",
    "        img = Image.open(img_fp)\n",
    "\n",
    "        features = self.pil_to_tensor(img)\n",
    "        features = self.resize(features)\n",
    "        #features = self.rgbify(features)\n",
    "\n",
    "        return features, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335d1a2c-34e5-4d3a-9be1-2706e4cb014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates a validation dataset class\n",
    "class ItemsValDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.examples = self._load_examples()\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((225,225))\n",
    "        #self.rgbify = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)!=1 else x)\n",
    "\n",
    "    def _load_examples(self):\n",
    "        class_names = os.listdir('pytorch_images_tv_split_2/val')\n",
    "        class_encoder = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        class_decoder = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "        examples_list = []\n",
    "        \n",
    "        for cl_name in class_names:\n",
    "            example_fp = os.listdir(os.path.join('pytorch_images_tv_split_2/val',cl_name))\n",
    "            example_fp = [os.path.join('pytorch_images_tv_split_2/val', cl_name, img_name ) for img_name in example_fp]\n",
    "            example = [(img_name, class_encoder[cl_name]) for img_name in example_fp]\n",
    "            examples_list.extend(example)\n",
    "\n",
    "        return examples_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fp, img_class = self.examples[idx]\n",
    "        img = Image.open(img_fp)\n",
    "\n",
    "        features = self.pil_to_tensor(img)\n",
    "        features = self.resize(features)\n",
    "        #features = self.rgbify(features)\n",
    "\n",
    "        return features, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5598f647-b593-4dd2-acd8-7040c6ec2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = ItemsTrainDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d3b971-a200-4410-aa28-ba442b840b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5754c01e-76ca-452a-8110-6f3b2aa8f9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdataset = ItemsValDataSet()\n",
    "len(valdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060b5233-c30a-4fbe-aa11-17ce9f3e7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Created a classifier based on the RESNET50 pretrained model\n",
    "\n",
    "class ItemClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "        #self.resnet50 = model\n",
    "        self.resnet50.fc = torch.nn.Linear(2048,13)\n",
    "  \n",
    "    def forward(self, X):\n",
    "        return F.softmax(self.resnet50(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd79aea-afa0-4142-b5c5-8e141a7a223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,traindataloader, valdataloader, epochs):\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    model_path = str(os.path.join('model_evaluation', time.strftime(\"%Y%m%d-%H%M%S\")))   \n",
    "    os.makedirs(model_path)\n",
    "    os.makedirs(os.path.join(model_path, 'weights'))\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        tr_num_correct = 0\n",
    "        tr_num_examples = 0\n",
    "        epoch_combo = 'epoch' + str(epoch)\n",
    "        os.makedirs(os.path.join(model_path, 'weights', epoch_combo))\n",
    "        for inputs, labels in traindataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            #print(predictions.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            model_save_dir = str(os.path.join(model_path, 'weights', epoch_combo, 'weights.pt'))\n",
    "            full_path = str('/home/jupyter/facebook-marketplaces-recommendation-ranking-system/Practicals')\n",
    "            #print(model_save_dir)\n",
    "            #torch.save({'epoch': epoch,\n",
    "            #    'model_state_dict': model.state_dict(),\n",
    "            #    'optimizer_state_dict': optimiser.state_dict()}, \n",
    "                  #str(os.path.join(full_path, model_save_dir)))\n",
    "            #           model_save_dir)\n",
    "            torch.save({'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimiser.state_dict()},\n",
    "                  str(os.path.join(full_path, model_save_dir)))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            #batch_idx += 1\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\n",
    "            tr_num_correct += torch.sum(correct).item()\n",
    "            tr_num_examples += correct.shape[0]\n",
    "        training_loss /= len(traindataloader.dataset)\n",
    "        training_accuracy = tr_num_correct / tr_num_examples\n",
    "        ## add training performance to tensorboard\n",
    "        writer.add_scalar('Training Loss', training_loss, global_step)\n",
    "        writer.add_scalar('Training Accuracy', training_accuracy, global_step)\n",
    "\n",
    "        model.eval()\n",
    "        val_num_correct = 0\n",
    "        val_num_examples = 0\n",
    "        for inputs, labels in valdataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim =1), dim=1)[1], labels)\n",
    "            val_num_correct += torch.sum(correct).item()\n",
    "            val_num_examples += correct.shape[0]\n",
    "        validation_loss /= len(valdataloader.dataset)\n",
    "        validation_accuracy = val_num_correct / val_num_examples\n",
    "        ## add validation performance to tensorboard\n",
    "        writer.add_scalar('Validation Loss', validation_loss, global_step)\n",
    "        writer.add_scalar('Validation Accuracy', validation_accuracy, global_step)\n",
    "        perf_dict = {}\n",
    "        perf_dict[epoch] = {'training_loss': training_loss,\n",
    "                            'val_loss': validation_loss,\n",
    "                            'training_accuracy': tr_num_correct / tr_num_examples,\n",
    "                            'val_accuracy': val_num_correct / val_num_examples}\n",
    "                            \n",
    "                            \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f},val_accuracy = {:.2f} '.format(epoch, training_loss, validation_loss, tr_num_correct / tr_num_examples,\n",
    "                                                                                                                             val_num_correct / val_num_examples))\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de356da2-5cf0-4998-9293-b7b218379543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d91ba037-56f2-4878-86d3-14033352b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "classifier = ItemClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d2b4b8d-c182-4ab5-9150-825557184273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## unfreeze last two layers\\nfor param in classifier.resnet50.layer3:\\n  param.requires_grad=True\\n\\nfor param in classifier.resnet50.layer4:\\n  param.requires_grad=True\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## unfreeze last two layers\n",
    "for param in classifier.resnet50.layer3:\n",
    "  param.requires_grad=True\n",
    "\n",
    "for param in classifier.resnet50.layer4:\n",
    "  param.requires_grad=True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "088dc544-bbd6-4465-9cb0-84a3c709e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## define the layers to unfreeze and then retrain\n",
    "layers_to_unfreeze = ['layers.2', 'layers.3']\n",
    "\n",
    "for name, param in classifier.resnet50.named_parameters():\n",
    "    for layer_name in layers_to_unfreeze:\n",
    "        if layer_name in name:\n",
    "            param.requires_grad = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855620e-03fd-439e-bd15-5aa9960d2d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.10,val_accuracy = 0.11 \n"
     ]
    }
   ],
   "source": [
    "#train_dataset = ItemsTrainDataSet()\n",
    "#val_dataset = ItemsValDataSet()\n",
    "train_loader = DataLoader(dataset = traindataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset = valdataset, batch_size=16)\n",
    "train(classifier, traindataloader= train_loader, valdataloader= val_loader, epochs=150)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
